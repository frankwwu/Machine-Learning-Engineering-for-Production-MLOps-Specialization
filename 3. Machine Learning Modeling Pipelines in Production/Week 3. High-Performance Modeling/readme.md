# Learning Objectives

* Identify the rise in computational requirements with current network architectures
* Select techniques to optimize the usage of computational resources to best serve your model needs
* Carry out high-performance data ingestion to reduce hardware accelerators idling time
* Distinguish between data and model parallelism to train your models in the most efficient way
* Implement knowledge distillation to reduce models that capture complex relationships among features so that they fit in constrained deployment environments

# High-Performance Modeling

Video: LectureDistributed Training
. Duration: 10 minutes10 min
Lab: Distributed Strategies with TF and Keras
. Duration: 1 hour1h
Video: LectureHigh-Performance Ingestion
. Duration: 11 minutes11 min
Video: LectureTraining Large Models - The Rise of Giant Neural Nets and Parallelism
. Duration: 13 minutes13 min
Reading: High-Performance Modeling
. Duration: 5 minutes5 min
Graded External Tool: Graded External ToolDistributed Multi-worker TensorFlow Training on Kubernetes
. Duration: 1 hour 30 minutes1h 30m
Due Aug 23, 2:59 AM EDT
Practice Quiz: High-Performance Modeling
6 questions

# Knowledge Distillation

Video: LectureTeacher and Student Networks
. Duration: 3 minutes3 min
Video: LectureKnowledge Distillation Techniques
. Duration: 9 minutes9 min
Lab: Knowledge Distillation
. Duration: 1 hour1h
Video: LectureCase Study - How to Distill Knowledge for a Q&A Task
. Duration: 8 minutes8 min
Reading: Knowledge Distillation
. Duration: 2 minutes2 min
Practice Quiz: Knowledge Distillation
7 questions